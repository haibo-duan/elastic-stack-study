# Task 1
将a字段定义为nested,将原始idx索引reindex到新索引, 确认对 nested object查询的结果

数据准备:
```
PUT /order/_doc/1
{
  "order_name": "xiaomi order",
  "desc": "shouji zhong de zhandouji",
  "goods_count": 3,
  "total_price": 12699,
  "goods_list": [
    {
      "name": "xiaomi PRO MAX 5G",
      "price": 1999
    },
    {
      "name": "ganghuamo",
      "price": 19
    },
    {
      "name": "shoujike",
      "price": 1999
    }
  ]
}
PUT /order/_doc/2
{
  "order_name": "Cleaning robot order",
  "desc": "shouji zhong de zhandouji",
  "goods_count": 2,
  "total_price": 12699,
  "goods_list": [
    {
      "name": "xiaomi cleaning robot order",
      "price": 1999
    },
    {
      "name": "dishwasher",
      "price": 4999
    }
  ]
}

```
现在查询，goods_list.name 为xiaomi goods_list.price为4999 会查询到数据。现在需要将这个查询弄正确。
```
GET order/_search
{
  "query": {
    "bool": {
      "must": [
        {
          "match": {
            "goods_list.name": "xiaomi"
          }
        },
        {
          "term": {
            "goods_list.price": {
              "value": 4999
            }
          }
        }
      ]
    }
  }
}
```

答案：
将数据reindex到 order_new 
```
DELETE order_new
# 1.修改mapping将类型改为nested
PUT order_new
{
  "mappings": {
    "properties": {
      "goods_list" : {
        "type": "nested"
      }
    }
  }
}

# 2.reindex索引
POST _reindex
{
  "source": {
    "index": "order"
  },
  "dest": {
    "index": "order_new"
  }
}

# 3.检索数据
GET order_new/_search
{
  "query": {
    "bool": {
      "must": [
        {
          "match": {
            "goods_list.name": "xiaomi"
          }
        },
        {
          "term": {
            "goods_list.price": {
              "value": 4999
            }
          }
        }
      ]
    }
  }
}
```

# Task 2
现有索引index_synonym，用oa、OA、oA match查询是3条，使用dingding的match查询是2条。
- 要求

- 使用_reindex创建新的索引到index_synonyms_reindex
使用oa、OA、0A、oA、dingding都是6条
- 考点

自定义分词器
同义词
reindex
文档

- 数据
```
PUT index_synonym/_bulk
{"index":{"_id":1}}
{"title":"oa is very good"}
{"index":{"_id":2}}
{"title":"oA is very good"}
{"index":{"_id":3}}
{"title":"OA is very good"}
{"index":{"_id":4}}
{"title":"dingding is very good"}
{"index":{"_id":5}}
{"title":"dingding is ali software"}
{"index":{"_id":6}}
{"title":"0A is very good"}
PUT index_synonym/_bulk
{"index":{"_id":1}}
{"title":"oa is very good"}
{"index":{"_id":2}}
{"title":"oA is very good"}
{"index":{"_id":3}}
{"title":"OA is very good"}
{"index":{"_id":4}}
{"title":"dingding is very good"}
{"index":{"_id":5}}
{"title":"dingding is ali software"}
{"index":{"_id":6}}
{"title":"0A is very good"}
```

答案
```
# 1.自定义分词器 + mapping  定义在字段级

PUT index_synonym_reindex
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "type": "custom",
          "tokenizer": "whitespace",
          "filter": [
            "my_synonym"
          ]
        }
      },
      "filter": {
        "my_synonym": {
          "type": "synonym",
          "synonyms": [
            "oa,OA,Oa,oA,0a,dingding"
          ]
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "title": {
        "type": "text",
        "analyzer": "my_analyzer"
      }
    }
  }
}

#2. reindex数据
POST _reindex
{
  "source": {
    "index": "index_synonym"
  },
  "dest": {
    "index": "index_synonyms_reindex"
  }
}

#3.查询
GET index_synonym_reindex/_search
{
  "query": {
    "match": {
      "title": "0a"
    }
  }
}

```

# Task 3
在路径/home/elastic/backups建立快照目录myrepo, 对索引movie创建快照movie_1
考点
● snapshot
● restore

答案:
```

```

# Task 4
目前有个索引是task4，用oa、OA、Oa phrase查询是3条，使用dingding的phrase查询是2条，通过reindex 索引后能够使得使用 oa、OA、Oa、oA、0A、dingding都是6条。
```
# 数据
PUT task4/_bulk
 {"index":{"_id":1}}
 {"title":"oa is very good"}
 {"index":{"_id":2}}
 {"title":"oA is very good"}
 {"index":{"_id":3}}
 {"title":"OA is very good"}
 {"index":{"_id":4}}
 {"title":"dingding is very good"}
 {"index":{"_id":5}}
 {"title":"dingding is ali software"}
 {"index":{"_id":6}}
 {"title":"0A is very good"}

# 在setting中定义分词器 处理同义词 然后在mapping中进行配置
PUT task4_new
{
  "settings": {
    "index": {
      "analysis": {
        "analyzer": {
          "my_synonym": {
            "tokenizer": "whitespace",
            "filter": [
              "synonym"
            ]
          }
        },
        "filter": {
          "synonym": {
            "type": "synonym",
            "synonyms": [
              "oa,OA,Oa,oA,0A,dingding"
            ]
          }
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "title": {
        "type": "text",
        "analyzer": "my_synonym"
      }
    }
  }
}

# 3. reindex

POST _reindex
{
  "source": {
    "index": "task4"
  },
  "dest": {
    "index": "task4_new"
  }
}
 
# 4.查询
GET task4_new/_search
{
  "query": {
    "match_phrase": {
      "title": "dingding"
    }
  }
}

GET task4_new/_search
{
  "query": {
    "match_phrase": {
      "title": "oa"
    }
  }
} 
```

# Task 5
现有的索引task5中，每个文档都有value01、value02、value03 这三个字段。为这个索引中的所有文档，新增一个字段，名称为domain，这个字段的值，是value01、value02、value03这三个字段的和(拼接)，每个值用.隔开。

```
# 1.准备数据
PUT task5/_doc/1
{
  "value01":"elastic",
  "value02":"org",
  "value03":"cn"
}
PUT task5/_doc/2
{
  "value01":"msb",
  "value02":"tech",
  "value03":"com"
}
# 2.定义pipline 为 set_domain
PUT _ingest/pipeline/set_domain
{
  "processors": [
    {
      "set": {
        "field": "domain",
        "value": "{{{value01}}}.{{{value02}}}.{{{value03}}}"
      }
    }
  ]
}
# 3.reindex 索引
POST _reindex
{
  "source": {
    "index": "task5"
  },
  "dest": {
    "index": "task5_new",
    "pipeline": "set_domain"
  }
}

# 4.查询数据
GET task5_new/_search
{
  "query": {
    "match_all": {}
  }
}

```

# Task 6
部署一个三节点集群，分别为node1，node2和node3
要求
● 集群中有index_a和index_b两个索引
● 每个索引有3个primary shards
● 将索引index_a所有的shards分配到node1，
● 索引index_b的所有shards分配到node2、node3
考点
● shard allocation awareness
● index allocation
文档
● Set up Elasticsearch> Configuring Elasticsearch > Cluster-level shard allocation and routing setting
● Index modules > Index Shard Allocation > Index-level shard allocation filtering

```
GET _cat/shards/food-ingredient

PUT food-ingredient/_settings
{
  "number_of_replicas": 0, 
  "index.routing.allocation.require._name": "node203",
  "index.routing.allocation.require.rack": "r1"
}

GET food-ingredient/_refresh

GET food-ingredient/_settings
```

# Task 7
索引task7为食品的配料索引，ingredient字段包含了配料信息，
manufacturer字段是对应的供应商信息
- 要求
● 请查询生产食品配料中包含C3H6N6数量前三的食品的供应商

- 数据
```
POST task7/_bulk
{"index":{"_id":1}}
{"ingredient":"first movie is msb show ","manufacturer":"TX"}
{"index":{"_id":2}}
{"ingredient":"first movie is msb show","manufacturer":"TX"}
{"index":{"_id":3}}
{"ingredient":"first C3H6N6 is msb show","manufacturer":"TX"}
{"index":{"_id":4}}
{"ingredient":"first movie is show msb tech","manufacturer":"Ali"} 
{"index":{"_id":5}}
{"ingredient":"first C3H6N6 is msb show ","manufacturer":"Ali"}
{"index":{"_id":6}}
{"ingredient":"first C3H6N6 is msb show","manufacturer":"Ali"}
{"index":{"_id":7}}
{"ingredient":"first C3H6N6 is msb show","manufacturer":"Bytes"}
{"index":{"_id":8}}
{"ingredient":"first C3H6N6 is show msb tech","manufacturer":"Bytes"}
{"index":{"_id":9}}
{"ingredient":"first C3H6N6 is msb show ","manufacturer":"Bytes"}
{"index":{"_id":10}}
{"ingredient":"first C3H6N6 is msb show","manufacturer":"beautiful_group"}
{"index":{"_id":11}}
{"ingredient":"first C3H6N6 is msb show","manufacturer":"didi"}
{"index":{"_id":12}}
{"ingredient":"first movie is show msb tech","manufacturer":"beautiful_group"} 

# 2.定义查询
GET task7/_search
{
  "size": 0,
  "query": {
    "match": {
      "ingredient": "C3H6N6"
    }
  },
  "aggs": {
    "term_manufacturer": {
      "terms": {
        "field": "manufacturer.keyword",
        "size": 3
      }
    }
  }
}
```

# Task 8
基于食品原材料的题目，
要求
● title字段中包含xxx或者sss，
● 结果先按照field_a字段正序，再按照评分倒序
● title中高亮匹配结果，并且用b标签嵌套

```
# 方法一
GET task7/_search
{
  "query": {
    "bool": {
      "should": [
        {
         "match": {
           "ingredient": "first"
         }
        },
        {
          "match": {
            "ingredient": "msb"
          }
        }
      ]
    }
  },
  "sort": [
    {
      "ingredient.keyword": {
        "order": "asc"
      }
    },
    {
      "_score": {
        "order": "desc"
      }
    }
  ],
  "highlight": {
    "fields": {
      "ingredient": {
        "pre_tags": "<b>",
        "post_tags": "</b>"
      }
    }
  }
}
# 方法二 利用match分词
GET task7/_search
{
  "query": {
   "match": {
     "ingredient": "first msb"
   }
  },
  "sort": [
    {
      "ingredient.keyword": {
        "order": "asc"
      }
    },
    {
      "_score": {
        "order": "desc"
      }
    }
  ],
  "highlight": {
    "fields": {
      "ingredient": {
        "pre_tags": "<b>",
        "post_tags": "</b>"
      }
    }
  }
}
```

# Task 9
索引task9是包含title字段，title字段是包含空格的多个名称组成的短语，新建一个索引，为索引增加包含原索引title的多个字段，每个字段的值是把title按照空格拆分后的值。

```
# 1.数据
PUT task9/_doc/1
{
  "title":"msb tech elastic certified engineer"
}

# 2.查询
GET task9/_search
{
  "query": {
    "match_all": {}
  }
}

# 3.定义pipline 用script处理
PUT _ingest/pipeline/my_title_pipline
{
  "processors": [
    {
      "script": {
        "lang": "painless",
        "source": """
            String str = ctx['title'];
            String[] arry = str.splitOnToken(params['delimiter']);
            for(int i=0;i<arry.length;i++){
              ctx['tag'+i] = arry[i];
            }
          """,
        "params": {
          "delimiter": " "
        }
      }
    }
  ]
}

# 4.reindex索引
POST _reindex
{
  "source": {
    "index": "task9"
  },
  "dest": {
    "index": "task9_new",
    "pipeline": "my_title_pipline"
  }
}

# 5.查询新索引
GET task9_new/_search
{
  "query": {
    "match_all": {}
  }
}

```

# Task 10
earthquakes索引中包含了过去11个月的地震信息，请通过一句查询，获取以下信息
要求
● 过去11个月，每个月的平均 地震等级（magiitude）
● 过去11个月里，平均地震等级最高的一个月及其平均地震等级
● 搜索结果不要包含任何文档
考点
● pipeline
● date_histogram

```
GET earthquakes/_search
{
  "size": 0, 
  "aggs": {
    "avg_magiitude_per_month": {
      "date_histogram": {
        "field": "time",
        "interval": "month"
      },
      "aggs": {
        "avg_magiitude": {
          "avg": {
            "field": "magnitude"
          }
        }
      }
    }
    ,
    "max_month_magnitude": {
      "max_bucket": {
        "buckets_path": "avg_magiitude_per_month>avg_magiitude"
      }
    }
  }
}
```

# Task 11
按照要求写一个search template

● 写入search template，名字为ser_temp_1
● 根据search template写出相应的match query，有my_field、my_value、size字段
● 通过参数来调用这个search template
● 这个search template运用到 kibana_sample_data_flights中
● my_field是DestCountry，而my_value是CN，size为5.
答案

```
# 1.定义search template
PUT _scripts/ser_temp_1
{
  "script": {
    "lang": "mustache",
    "source": {
      "query": {
        "match": {
          "{{my_field}}": "{{my_value}}"
        }
      },
      "size": "{{size}}"
    }
  }
}

# 2.应用查询的template
GET kibana_sample_data_flights/_search/template
{
 "id": "ser_temp_1",
  "params": {
    "my_field": "DestCountry",
    "my_value": "CN",
    "size": 5
  }
}
```

# Task 12

已有索引task12，里面有个对象类型的字段user，这个user里面还是个数组字段，分为user.first和user.last。先要求定义一个新的索引task12_new中，user字段类型为nested类型，然后从task12中reindex数据到task12_new中，使用nested的查询
答案
```
# 1.准备数据
PUT task12/_doc/1
{
  "user": [
    {
      "first": "lei",
      "last": "wu"
    },
    {
      "first": "bee",
      "last": "new"
    }
  ]
}

2.定义nested mapping
PUT task12_new
{
  "mappings": {
    "properties": {
      "user": {
        "type": "nested"
      }
    }
  }
}
# 3.reindex
POST _reindex
{
  "source": {
    "index": "task12"
  },
  "dest": {
    "index": "task12_new"
  }
}
#4.nested查询
GET task12_new/_search
{
  "query": {
    "nested": {
      "path": "user",
      "query": {
        "bool": {
          "must": [
            {
              "term": {
                "user.first": {
                  "value": "lei"
                }
              }
            },
            {
              "term": {
                "user.last": {
                  "value": "wu"
                }
              }
            }
          ]
        }
      }
    }
  }
}
```

# Task 13

创建索引模板my_index_template
要求
● 只要名字以msb_开头的index ，都用这个mappings
● 设置主分片1，副本数量0
● int_开头的字段都是用integer类型
● 字符串类型的字段都使用text类型并且使用english分词器
答案
```
# 1.定义template
PUT _index_template/my_index_template
{
  "index_patterns": [
    "msb*"
  ],
  "template": {
    "settings": {
      "number_of_replicas": 0,
      "number_of_shards": 1
    },
    "mappings": {
      "dynamic_templates": [
        {
          "text_analyzer": {
            "match_mapping_type": "string",
            "mapping": {
              "type": "text",
              "analyzer": "english"
            }
          }
        },
        {
          "longs_as_strings": {
            "match": "int_*",
            "mapping": {
              "type": "integer"
            }
          }
        }
      ]
    }
  }
}
# 2.写入数据
POST msb_index_template/_doc
{
  "int_field":"2",
  "string_field":"sadsdads"
}
# 3.查询
GET msb_index_template/_mapping
```

# Task 14
设定一个index alias名字为alias2，默认查询只返回评分大于3的电影。
答案：
```
POST _aliases
{
  "actions": [
    {
      "add": {
        "index": "alias",
        "alias": "alias2",
        "filter": {
          "range": {
            "score": {
              "gte": 3
            }
          }
        }
      }
    }
  ]
}
GET alias2/_search
```

# Task 15

There is a Kibana instance configured for cluster2 that is running on port 5601. Write a single search of the movies index on cluster2 that satisfies the following requirements

● the overview field must contain the phrase "msb show"
● at least 2 of the title, tags, or tagline fields must be a match for the phrase "msb show"

In the text field below, provide only the JSON portion of your search request. 

```
GET movies/_search
{
  "query": {
    "bool": {
      "must": [
        {
          "match_phrase": {
            "overview": "msb show"
          }
        }
      ],
      "should": [
        {
          "match_phrase": {
            "title": "msb show"
          }
        },
        {
          "match_phrase": {
            "tags": "msb show"
          }
        },
        {
          "match_phrase": {
            "tagline": "msb show"
          }
        }
      ],
      "minimum_should_match": 2
    }
  }
}
```

# Task 16
有一个文档，内容类似dog & cat， 要求索引这条文档，并且使用match_phrase query，查询dog & cat或者dog and cat都能match。
```
# 1.准备数据
POST task16/_doc/1
{
  "title": "dog & cat"
}

POST task16/_doc/2
{
  "title": "dog and cat"
}

GET task16/_search
{
  "query": {
    "match_phrase": {
      "title": "dog & cat"
    }
  }
}
# 2.定义同义词 分析器和mapping
DELETE task16_new
PUT task16_new
{
  "settings": {
    "index": {
      "analysis": {
        "analyzer": {
          "my_synonym": {
            "tokenizer": "whitespace",
            "filter": [
              "synonym"
            ]
          }
        },
        "filter": {
          "synonym": {
            "type": "synonym",
            "lenient": true,
            "synonyms": [
              "&,and"
            ]
          }
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "title":{
        "type": "text",
        "analyzer": "my_synonym"
      }
    }
  }
}

# 3.reindex数据
POST _reindex
{
  "source": {
    "index": "task16"
  },
  "dest": {
    "index": "task16_new"
  }
}

# 4查询数据
GET task16_new/_search
{
  "query": {
    "match_phrase": {
      "title": "dog and cat"
    }
  }
}

```

# task 17
对多个索引定义相同的别名，并指定其中的一个索引能写入文档数据。
```
# 1.准备数据
POST task17_1/_doc/1
{
  "title": "task17_1"
}
POST task17_2/_doc/1
{
  "title": "task17_2"
}

#2.定义别名
POST _aliases
{
  "actions": [
    {
      "add": {
        "index": "task17_1",
        "alias": "task17",
        "is_write_index": false
      }
    },
    {
      "add": {
        "index": "task17_2",
        "alias": "task17",
        "is_write_index": true
      }
    }
  ]
}
#3.重新插入数据
POST task17/_doc/3
{
  "title": "task17"
}

#4.查询数据
GET task17/_search
{
  "query": {
    "match_all": {}
  }
}

GET task17_2/_search
{
  "query": {
    "match_all": {}
  }
}

```

# task18
为集群cluster_one和cluster_two配置跨集群搜索。

● 这两个集群中，都有索引kibana_sample_data_flights，同时对这两个集群中的kibana_sample_data_flights索引，进行检索。

```
PUT _cluster/settings
{
  "transient": {
    "cluster": {
      "remote": {
        "c1": {
          "seeds": [
            "192.168.162.201:9200"
          ]
        },
        "c2": {
          "seeds": [
            "192.168.161.114:9200"
          ]
        }
      }
    }
  }
}

GET /c1:kibana_sample_data_flights,c2:kibana_sample_data_flights/_search
{
  "query": {
    "match_phrase": {
      "Dest": "Sydney Kingsford Smith International Airport"
    }
  }
}
```

# Task 19

Suppose you have documents that look like the following:
```
{
"username": "elastic.org.cn",
"address":{
"city": "Mountain View",
"state": "California",
"country": "United States of America"
},
"comment": "To be prepared for the exam, you should be able to complate all of the exam object"
}
```
A Kibana instance for cluster2 is running on port 5602. Define a new index on cluster2 that satisfies the following requirements:

● The name of the index is task19
● The username field is mapped as a keyword only
● The field in address are all mapped as both text and keyword
● The comment field is indexed as text three different ways: using the standard analyzer, the english analyzer, and the dutch analyzer
● Index the document above into your new task19 index with an _id of 123.

```
# 1.通过mapping 定义字段和分词器
PUT task19
{
  "mappings": {
    "properties": {
      "username": {
        "type": "keyword"
      },
      "address": {
        "properties": {
          "city": {
            "type": "text",
            "fields": {
              "keyword": {
                "type": "keyword",
                "ignore_above": 256
              }
            }
          },
          "country": {
            "type": "text",
            "fields": {
              "keyword": {
                "type": "keyword",
                "ignore_above": 256
              }
            }
          },
          "state": {
            "type": "text",
            "fields": {
              "keyword": {
                "type": "keyword",
                "ignore_above": 256
              }
            }
          }
        }
      },
      "comment": {
        "type": "text",
        "analyzer": "standard",
        "fields": {
          "english": {
            "type": "text",
            "analyzer": "english"
          },
          "dutch": {
            "type": "text",
            "analyzer": "dutch"
          }
        }
      }
    }
  }
}

# 2.插入数据
PUT task19/_doc/123
{
  "username": "elastic.org.cn",
  "address":{
  	"city": "Mountain View",
  	"state": "California",
  	"country": "United States of America"
  },
  "comment": "To be prepared for the exam, you should be able to complate all of the exam object"
}

# 3.查询mapping 搜索数据
GET task19/_mapping
GET task19/_search
```

# Task20
task20的是三个字段a,b,c进行查询，要求字段c的boost为2，各个字段的查询算法加和

```
GET task20/_search
{
  "query": {
    "multi_match": {
      "query": "a",
      "type": "most_fields", 
      "fields": ["a","b","c^2"]
    }
  }
}

```

Task 21
一篇文档，字段内容包括了 “hello & world”，索引后，要求使用 match_phrase query
要求
● 查询 hello & world 或者 hello and world 都能匹配
```
# 1.定义测试数据
PUT task21/_doc/1
{
  "title":"hello & world"
}

PUT task21/_doc/2
{
  "title":"hello and world"
}

# 2.定义setting和mapping 用同义词分词器
PUT task21_new
{
  "settings": {
    "index": {
      "analysis": {
        "analyzer": {
          "my_synonym": {
            "tokenizer": "whitespace",
            "filter": [
              "synonym"
            ]
          }
        },
        "filter": {
          "synonym": {
            "type": "synonym",
            "lenient": true,
            "synonyms": [
              "foo, bar => baz"
            ]
          }
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "title":{
        "type": "text",
        "analyzer": "my_synonym"
      }
    }
  }
}

# 3.执行reindex
POST _reindex
{
  "source": {
    "index": "task21"
  },
  "dest": {
    "index": "task21_new"
  }
}
# 4.查询
GET task21_new/_search
{
  "query": {
    "match": {
      "title": "hello & world"
    }
  }
}
```

# Task 22
写一个查询，要求某个关键字new york在my_index这个索引中，4个字段("overview"/"title"/"tags"/"tagline")中至少包含两个以上
考点
● bool 查询，should / minimum_should_match

```
GET task22/_search
{
  "query": {
    "bool": {
      "should": [
        {
          "match": {
            "overview": "new york"
          }
        },
        {
          "match": {
            "title": "new york"
          }
        },
        {
          "match": {
            "tags": "new york"
          }
        },
        {
          "match": {
            "tagline": "new york"
          }
        }
      ],
      "minimum_should_match": 2
    }
  }
}
```

# Task 23

There is a Kibana instance configured for cluster2 that is running on port 5602. The earthquakes index on cluster2 contains details about earthquakes that occurred for 11 months from January to November of 2016. Write one search that returns both of the following:

● the average magnitude of earthquakes for each of the 11 months
● the month and average magnitude of the month with the largest average magnitude
● the search response does not contain any documents.

In the text field below, provide only the JSON portion of your search request.

```
GET earthquakes/_search
{
  "size": 0, 
  "aggs": {
    "avg_magnitude_per_month": {
      "date_histogram": {
        "field": "time",
        "interval": "month"
      },
      "aggs": {
        "avg_magnitude": {
          "avg": {
            "field": "magnitude"
          }
        }
      }
    },
    "max_month_avg":{
      "max_bucket": {
        "buckets_path": "avg_magnitude_per_month>avg_magnitude"
      }
    }
  }
}

```

# Task 24
索引index_a的字段tags（该字段是数组）里的每个子项都包含空格，创建一个新的索引index_b。
要求

● 新索引中tags字段中每个项都要去掉空格
● 增加一个新字段，这个新字段的值是index_a中每个tags字段中每个项的拼接（不包含空格）
考点
● pipeline
● script
● reindex

```
# 1.准备数据
PUT task24/_bulk
 {"index":{"_id":1}}
 {"tags":[" msb "," tech "]}
 {"index":{"_id":2}}
 {"tags":[" wl ", " teacher "]}
 
 # 2.定义pipline
 PUT _ingest/pipeline/my_task24_pipline
 {
   "processors": [
      {
        "script": {
          "lang": "painless",
          "source": """
            List list = ctx['tags'];
            String str = '';
            for(int i=0;i<list.size();i++) {
              ctx['tags'][i] = list.get(i).trim();
              str += list.get(i).trim();
            }
            ctx['tags_str'] = str;
          """
        }
      }
    ]
 }
 # 3.reindex数据
 POST _reindex
 {
   "source": {
     "index": "task24"
   },
   "dest": {
     "index": "task24_new",
     "pipeline": "my_task24_pipline"
   }
 }
 # 4.查询数据
 GET task24_new/_search
 {
   "query": {
     "match_all": {}
   }
 }
```

# Task 25

安装并配置 一个 hot & warm 架构的集群
要求
● 三个节点， node 1 为 hot ， node2 为 warm，node 3 为cold
● 三个节点均为 master-eligable 节点
● 新创建的索引，数据写入 hot 节点
● 通过一条命令，将数据从 hot 节点移动到 warm 节点
考点
● multi_match
● boost
● most_fields



# Task 26
此题为球友“哈密瓜”真题考试真题，题目中暗藏杀机，很容易把题目考点理解为"match"和"term"查询的考点
有如下索引：
```
POST inject/_doc/1
{
"title": "the number is error"
}
POST inject/_doc/2
{
"title": "it is number is error"
}
```
用下面的查询是返回一条数据
```
GET inject/_search
{
"query": {
"match": {
"title": "the"
}
}
}
```
要求
● 新建一个索引 named inject_new
● 通过 reindex将inject的数据和类型复制到 inject_new上
● 通过 term查询，term 'the' value of title，返回0条数据
考点
● mapping
● reindex

```
# 修改分词器
 PUT inject_new
 {
   "mappings": {
     "properties": {
       "title":{
         "type": "text",
         "analyzer": "english",
         "fields": {
           "keyword":{
             "type": "keyword",
             "ignore_above": 256
           }
         }
       }
     }
   }
 }
 # reindex
 POST _reindex
{
  "source": {
    "index": "inject"
  },
  "dest": {
    "index": "inject_new"
  }
}
# 查询
GET inject_new/_search
{
  "query": {
    "term": {
      "title": "the"
    }
  }
}
```

# Task 27

There is a Kibana instance configured for cluster2 that is running on port 5602. Write a single search on the movie_data index on cluster2 that satisfies the following requirements:

● The tags field containes "based on comic book" and "marvel cinematic universe"
● The results are sorted first by budget descending, then release_data descending

In the text field below, provide only the JSON portion of your search request:

```
GET movie_data/_search
{
  "query": {
    "bool": {
      "must": [
        {
          "match_phrase": {
            "tags": "based on comic book"
          }
        },
        {
          "match_phrase": {
            "tags": "marvel cinematic universe"
          }
        }
      ]
    }
  },
  "sort": [
    {
      "budget": {
        "order": "desc"
      }
    },
    {
      "release_data": {
        "order": "desc"
      }
    }
  ]
}
```

# Task 28

There is a Kibana instance configured for cluster2 that is running on port 5602. Define a pipeline to be used on the earthquakes index that satisfies the following requirements:

● The ID of the pipeline is earthquakes_pipeline
● The value of each magnitude_type field is uppercased
● If the document does not contain a field named batch_number, then a new batch_number field is added and set to 1
● If the document already contains a field named batch_number, then the batch_number field is incremented by 1
● After defining the pipeline, update all of the documents in the earthquakes index, applying your pipeline to each document during the updating.

```

PUT _ingest/pipeline/earthquakes_pipeline
{
  "processors": [
    {
      "uppercase": {
        "field": "magnitude_type"
      }
    },
    {
      "script": {
        "lang": "painless",
        "source": """
        if(ctx.containsKey('batch_number')) {
          ctx['batch_number'] += 1;
        }else {
           ctx['batch_number'] = 1;
        }
        """
      }
    }
  ]
}

GET earthquakes/_search
POST earthquakes/_update_by_query?pipeline=earthquakes_pipeline

```

# Task 29

在task29中包含一些文档，要求创建索引task29_new，通过reindex api将task29的文档索引到task29_new中。

要求
● 增加一个整型字段field_x，value是task29的title的字符长度；
● 增加一个数组类型的字段field_y，是value的词集合。
(field_y是空格分割的一组词，比如"foo bar"，索引到task29_new后，要求变成["foo","bar"])

```
# 1.定义pipeline
PUT _ingest/pipeline/my_task29_pipeline
{
  "processors": [
    {
      "script": {
        "lang": "painless",
        "source": """
          ctx['field_x'] = ctx['title'].length();
          String[] array = ctx['value'].splitOnToken(params['delimiter']);
          List list = new ArrayList();
          for(int i=0;i<array.length;i++) {
            if(array[i] != '') {
              list.add(array[i]);
            }
          }
           ctx['field_y'] = list;
          """,
        "params": {
          "delimiter": " "
        }
      }
    }
  ]
}
# 2 reindex数据
POST _reindex
{
  "source": {
    "index": "task29"
  },
  "dest": {
    "index": "task29_new",
    "pipeline": "my_task29_pipeline"
  }
}

# 3.查询
GET task29_new/_search
{
  "query": {
    "match_all": {}
  }
}

```

# Task 30
给定两个索引，earthquakes和 magnitude_type_desc。这两个所以里有一个共同的字段，并且是关联字段 magnitude_type，第二个索引里的第二个字段是desc，用来描述地震类型。

要求
● 创建一个新的索引，里面包含了earthquakes里面的全部数据，
● 每一条数据都要根据地震类型添加一个新的字段，就是desc。
考点
● pipeline processers
● enrich processer
● reindex

```
POST earthquakes/_bulk
{"index":{}}
{"country":"Title A","magnitude":"yyy","magnitude_type":"aaa"}
{"index":{}}
{"country":"Title B","magnitude":"yyy","magnitude_type":"bbb"}
{"index":{}}
{"country":"Title D","magnitude":"yyy","magnitude_type":"ddd"}
{"index":{}}
{"country":"Title E","magnitude":"yyy","magnitude_type":"eee"}
{"index":{}}
{"country":"Title E","magnitude":"yyy","magnitude_type":"hhh"}

POST magnitude_type_desc/_bulk
{"index":{}}
{"magnitude_type":"aaa","desc":"this is aaa"}
{"index":{}}
{"magnitude_type":"bbb","desc":"this is bbb"}
{"index":{}}
{"magnitude_type":"ccc","desc":"this is ccc"}
{"index":{}}
{"magnitude_type":"eee"}
{"index":{}}
{"magnitude_type":"hhh","desc":null}
```

答案：
```
# 1.定义policy
PUT /_enrich/policy/task30-policy
{
  "match": {
    "indices": "magnitude_type_desc_30",
    "match_field": "magnitude_type",
    "enrich_fields": ["desc"]
  }
}
# 2.编译policy
POST /_enrich/policy/task30-policy/_execute

# 3.定义pipeline
PUT /_ingest/pipeline/task30_pipeline
{
  "processors" : [
    {
      "enrich" : {
        "policy_name": "task30-policy",
        "field" : "magnitude_type",
        "target_field": "root",
        "max_matches": "1"
      },
      "set": {
        "field": "desc",
        "value": "{{root.desc}}"
      },
      "remove": {
        "field": "root",
        "ignore_failure": true
      }
    }
  ]
}
# 4.reindex
POST _reindex
{
  "source": {
    "index": "earthquakes_30"
  },
  "dest": {
    "index": "earthquakes_30_new",
     "pipeline": "task30_pipeline"
  }
}
# 5.查询数据
GET earthquakes_30_new/_search
{
  "query": {
    "match_all": {}
  }
}

```